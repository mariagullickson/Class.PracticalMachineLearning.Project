Predicting Exercise Styles
==========================

The goal of this project is to use accelerometer data from 6 participants while
they exercies to predict the manner in which they are exercising.

First we load up the data and split it into a training set (75%) and a test set (25%).
```{r loadData, cache=TRUE}
library(caret, quietly=TRUE)
data <- read.csv('pml-training.csv')
inTraining <- createDataPartition(data$classe, p=0.75, list=FALSE)
training <- data[inTraining,]
testing <- data[-inTraining,]
```

We are interested in predicting based on acceleromater data, so next we limit our
testing and training predictors to just those items.  Once we've pulled out the
predictors and outcomes we care about, we can remove the original full data set to
save on a bit of memory.
```{r limitData, cache=TRUE}
trainOutcome <- training[, names(training) == "classe"]
trainAccelerometers <- training[, grepl("^accel", names(training))]
testOutcome <- testing[, names(testing) == "classe"]
testAccelerometers <- testing[, grepl("^accel", names(testing))]
rm(data)
```

Now that our data is in shape, we can try out some modeling.  We will test out several
models, using 5-fold cross validation repeating with 3 full sets of folds.  This reduces the
variance in our results, and validates that we aren't overfitting our model to the
test data.
```{r fitSetup, cache=TRUE}

library(survival, quietly=TRUE)
library(gbm, quietly=TRUE)
library(splines, quietly=TRUE)
fitControl <- trainControl(method="repeatedcv", number=4, repeats=3)
```

First we will try out a Linear Discriminant Analysis Model (LDA).
```{r lda, cache=TRUE}
ldaFit <- train(trainOutcome ~ ., data=trainAccelerometers, method="lda",
                trControl=fitControl, verbose=FALSE)
ldaPredictions <- predict(ldaFit, testAccelerometers)
ldaCM <- confusionMatrix(ldaPredictions, testOutcome)
ldaAccuracy <- ldaCM$overall['Accuracy'] * 100
ldaCM
```

Next we will try out a Generalize Boosted Regression Model (GBM).
```{r gbm, cache=TRUE}
gbmFit <- train(trainOutcome ~ ., data=trainAccelerometers, method="gbm",
                trControl=fitControl, verbose=FALSE)
gbmPredictions <- predict(gbmFit, testAccelerometers)
gbmCM <- confusionMatrix(gbmPredictions, testOutcome)
gbmAccuracy <- gbmCM$overall['Accuracy'] * 100
gbmCM
```

Then we will try out a Random Forest Model (RF).
```{r rf, cache=TRUE}
rfFit <- train(trainOutcome ~ ., data=trainAccelerometers, method="rf",
               trControl=fitControl, verbose=FALSE)
rfPredictions <- predict(rfFit, testAccelerometers)
rfCM <- confusionMatrix(rfPredictions, testOutcome)
rfAccuracy <- rfCM$overall['Accuracy'] * 100
rfCM
```

Now that we've got 3 possible models, we'll compare the accuracy of each of them
to see which is the best fit for this problem.

```{r modelSummary, cache=TRUE,fig.width=10, fig.height=5}
library(ggplot2, quietly=TRUE)
library(gridExtra, quietly=TRUE)
Model = c('LDA', 'GBM', 'RF')
Accuracy = c(ldaAccuracy, gbmAccuracy, rfAccuracy)
OutOfSampleErrorEstimate = 100-Accuracy
modelSummary = data.frame(Model, Accuracy, OutOfSampleErrorEstimate)
accuracyPlot <- ggplot(data=modelSummary, aes(x=Model, y=Accuracy, fill=Model)) + geom_bar(stat="identity") + xlab("Model") + ylab("Accuracy %") + ggtitle("Model Accuracy")
errorPlot <- ggplot(data=modelSummary, aes(x=Model, y=OutOfSampleErrorEstimate, fill=Model)) + geom_bar(stat="identity") + xlab("Model") + ylab("Out Of Sample Error %") + ggtitle("Model Error Estimate")
grid.arrange(accuracyPlot, errorPlot, ncol=2)
```

From this, we can see that the Random Forest model clearly has the highest accuracy (`r rfAccuracy`%),
and thus the lowest estimated out-of-sample error rate (`r 100-rfAccuracy`%).  This is the model we will
choose to use.
